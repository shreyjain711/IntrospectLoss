{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8fead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "from logging import config\n",
    "from datasets import load_dataset\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "random.seed(711)\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c51623fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_beavertails_subset(n_rows=2500, split='330k_train'):\n",
    "    # beavertails subset creation\n",
    "    beavertails_ds = load_dataset('PKU-Alignment/BeaverTails', split=split)\n",
    "    # Filter out prompts with less than 10 characters\n",
    "    beavertails_ds = [item for item in beavertails_ds if len(item['prompt']) >= 10]\n",
    "\n",
    "    # Define the 14 category keys\n",
    "    category_keys = [\n",
    "        \"animal_abuse\",\n",
    "        \"child_abuse\",\n",
    "        \"controversial_topics,politics\",\n",
    "        \"discrimination,stereotype,injustice\",\n",
    "        \"drug_abuse,weapons,banned_substance\",\n",
    "        \"financial_crime,property_crime,theft\",\n",
    "        \"hate_speech,offensive_language\",\n",
    "        \"misinformation_regarding_ethics,laws_and_safety\",\n",
    "        \"non_violent_unethical_behavior\",\n",
    "        \"privacy_violation\",\n",
    "        \"self_harm\",\n",
    "        \"sexually_explicit,adult_content\",\n",
    "        \"terrorism,organized_crime\",\n",
    "        \"violence,aiding_and_abetting,incitement\"\n",
    "    ]\n",
    "\n",
    "    # One-hot encode the category columns\n",
    "    for item in beavertails_ds:\n",
    "        cat_dict = item['category']\n",
    "        total_violated = 0\n",
    "        for key in category_keys:\n",
    "            item[key + \"_onehot\"] = int(cat_dict.get(key, False))\n",
    "            total_violated += item[key + \"_onehot\"]\n",
    "        item['total_violated'] = total_violated\n",
    "\n",
    "    total_violated_counts = Counter(item['total_violated'] for item in beavertails_ds)\n",
    "    total = sum(total_violated_counts.values())\n",
    "    proportional_split = {k: max(1, int(round(n_rows * v / total))) for k, v in total_violated_counts.items()}\n",
    "\n",
    "    # Group items by total_violated\n",
    "    grouped = defaultdict(list)\n",
    "    for item in beavertails_ds:\n",
    "        grouped[item['total_violated']].append(item)\n",
    "\n",
    "    # Sample according to proportional_split\n",
    "    sampled_beavertails_ds = []\n",
    "    for k, v in proportional_split.items():\n",
    "        if v > 0 and k in grouped:\n",
    "            sampled_beavertails_ds.extend(random.sample(grouped[k], min(v, len(grouped[k]))))\n",
    "\n",
    "    print(proportional_split, len(sampled_beavertails_ds))\n",
    "    return sampled_beavertails_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c925ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wildguard_subset(benign_count=1500, harm_count=2500, split=\"train\"):\n",
    "\n",
    "    # wildguard (polyguard) subset creation\n",
    "    wildguard_ds = load_dataset('allenai/wildguardmix', name=f'wildguard{split}', split=split)\n",
    "\n",
    "    # Convert to list of dicts if needed\n",
    "    if not isinstance(wildguard_ds, list):\n",
    "        wildguard_ds = [dict(row) for row in wildguard_ds]\n",
    "\n",
    "    # Find all unique subcategories\n",
    "    subcategory_values = set(item['subcategory'] for item in wildguard_ds)\n",
    "    n_subcats = len(subcategory_values)\n",
    "    target_per_subcat = {}\n",
    "    target_per_subcat['benign'] = benign_count\n",
    "    other_subcats = subcategory_values - {'benign'}\n",
    "    n_other = len(other_subcats)\n",
    "    per_other = harm_count // n_other if n_other > 0 else 0\n",
    "    for subcat in other_subcats:\n",
    "        target_per_subcat[subcat] = per_other\n",
    "\n",
    "    # Group by subcategory and then by (adversarial, prompt_harm_label)\n",
    "    grouped = defaultdict(lambda: defaultdict(list))\n",
    "    for item in wildguard_ds:\n",
    "        key = (item['adversarial'], item['prompt_harm_label'])\n",
    "        grouped[item['subcategory']][key].append(item)\n",
    "\n",
    "    sampled_wildguard_ds = []\n",
    "    for subcat in subcategory_values:\n",
    "        subcat_groups = grouped[subcat]\n",
    "        # Find the minimum count among all (adversarial, prompt_harm_label) combinations for this subcat\n",
    "        keys = list(subcat_groups.keys())\n",
    "        n_combos = len(keys)\n",
    "        if n_combos == 0:\n",
    "            continue\n",
    "        per_combo = (target_per_subcat[subcat]+n_combos-1) // n_combos\n",
    "        for key in keys:\n",
    "            group = subcat_groups[key]\n",
    "            n = min(per_combo, len(group))\n",
    "            sampled_wildguard_ds.extend(random.sample(group, n))\n",
    "        # If not enough, fill up to target_per_subcat with random from remaining\n",
    "        current = sum(min(per_combo, len(subcat_groups[key])) for key in keys)\n",
    "        if current < target_per_subcat[subcat]:\n",
    "            leftovers = []\n",
    "            for key in keys:\n",
    "                group = subcat_groups[key]\n",
    "                if len(group) > per_combo:\n",
    "                    leftovers.extend([item for item in group if item not in sampled_wildguard_ds])\n",
    "            n_needed = target_per_subcat - current\n",
    "            if leftovers:\n",
    "                sampled_wildguard_ds.extend(random.sample(leftovers, min(n_needed, len(leftovers))))\n",
    "\n",
    "    print(f\"Sampled {len(sampled_wildguard_ds)} rows from wildguard_ds\")\n",
    "    return sampled_wildguard_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98af5aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: 575, 0: 1115, 1: 625, 3: 180, 4: 4, 5: 1, 7: 1, 6: 1, 9: 1, 12: 1} 2504\n",
      "Sampled 3992 rows from wildguard_ds\n"
     ]
    }
   ],
   "source": [
    "sampled_beavertails_ds_train = get_beavertails_subset(n_rows=2500, split='330k_train')\n",
    "sampled_wildguard_ds_train = get_wildguard_subset(benign_count=1500, harm_count=2500, split=\"train\")\n",
    "xstest_ds_train = load_dataset('walledai/XSTest', split='test')\n",
    "\n",
    "gsm8k_ds = load_dataset('openai/gsm8k', split='train', name='main')\n",
    "sampled_gsm8k_ds_train = random.sample(list(gsm8k_ds), 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc7d595d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2504, 3992, 450, 1500, 8446)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sampled_beavertails_ds_train), len(sampled_wildguard_ds_train), len(xstest_ds_train), len(sampled_gsm8k_ds_train), len(sampled_beavertails_ds_train) + len(sampled_wildguard_ds_train) + len(xstest_ds_train) + len(sampled_gsm8k_ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbd46712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 249, 0: 440, 2: 233, 3: 76, 5: 1, 4: 2, 11: 1} 1002\n",
      "Sampled 2512 rows from wildguard_ds\n"
     ]
    }
   ],
   "source": [
    "sampled_beavertails_ds_test = get_beavertails_subset(n_rows=1000, split='330k_test')\n",
    "sampled_wildguard_ds_test = get_wildguard_subset(benign_count=1000, harm_count=1500, split=\"train\")\n",
    "# xstest_ds_train = load_dataset('walledai/XSTest', split='test')\n",
    "\n",
    "gsm8k_ds = load_dataset('openai/gsm8k', split='test', name='main')\n",
    "sampled_gsm8k_ds_test = random.sample(list(gsm8k_ds), 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53499d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_dataset(sampled_beavertails_ds, sampled_wildguard_ds, xstest_ds, sampled_gsm8k_ds):\n",
    "    combined_dict = {'PKU-Alignment/BeaverTails': sampled_beavertails_ds, \n",
    "                     'allenai/wildguardmix': sampled_wildguard_ds, \n",
    "                     'walledai/XSTest': None if xstest_ds is None else list(xstest_ds), \n",
    "                     'openai/gsm8k': sampled_gsm8k_ds}\n",
    "    combined_ds = []\n",
    "    harm_count = 0\n",
    "    for dataset_name, dataset in combined_dict.items():\n",
    "        if dataset is None: continue\n",
    "        print(f\"Processing dataset: {dataset_name} with {len(dataset)} items\")\n",
    "        newItem = {'source': dataset_name}\n",
    "        for item in dataset:\n",
    "            newItem['origin_data'] = json.dumps(item, ensure_ascii=False)\n",
    "            if dataset_name == 'PKU-Alignment/BeaverTails':\n",
    "                newItem['prompt'] = item['prompt']\n",
    "                newItem['is_safe'] = int(item['is_safe'])\n",
    "            elif dataset_name == 'allenai/wildguardmix':\n",
    "                newItem['prompt'] = item['prompt']\n",
    "                newItem['is_safe'] = 1-int(item['prompt_harm_label'].lower() == ('harmful'))\n",
    "            elif dataset_name == 'walledai/XSTest':\n",
    "                newItem['prompt'] = item['prompt']\n",
    "                newItem['is_safe'] = int(item['label'].lower() == ('safe'))\n",
    "            elif dataset_name == 'openai/gsm8k':\n",
    "                newItem['prompt'] = item['question']\n",
    "                newItem['is_safe'] = 1\n",
    "            harm_count += (1 - newItem['is_safe'])\n",
    "            combined_ds.append(copy.deepcopy(newItem))\n",
    "    print(f\"Combined dataset size: {len(combined_ds)}, counts: {Counter(item['is_safe'] for item in combined_ds)}\")\n",
    "    return combined_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee1b2213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: PKU-Alignment/BeaverTails with 2504 items\n",
      "Processing dataset: allenai/wildguardmix with 3992 items\n",
      "Processing dataset: walledai/XSTest with 450 items\n",
      "Processing dataset: openai/gsm8k with 1500 items\n",
      "Combined dataset size: 8446, counts: Counter({1: 4365, 0: 4081})\n"
     ]
    }
   ],
   "source": [
    "combined_ds_test = generate_dataset(sampled_beavertails_ds_train, sampled_wildguard_ds_train, xstest_ds_train, sampled_gsm8k_ds_train)\n",
    "with open('../RepExtraction/input_data/combined_8500.json', 'w', encoding='utf-8') as f:\n",
    "    for item in combined_ds_test:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0340044a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset size: 4014, counts: Counter({0: 2074, 1: 1940})\n"
     ]
    }
   ],
   "source": [
    "combined_ds_test = generate_dataset(sampled_beavertails_ds_test, sampled_wildguard_ds_test, None, sampled_gsm8k_ds_test)\n",
    "with open('../RepExtraction/input_data/combined_4000_test.json', 'w', encoding='utf-8') as f:\n",
    "    for item in combined_ds_test:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c98ca20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
